{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP/fibaNVfqJc4unmo2V30E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaifbilal/Neuronpedia_API_LLM/blob/main/Neuronpedia_API_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-zd4Cpjj0Or"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch sae-lens streamlit pyngrok requests --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --force-reinstall numpy\n",
        "!pip install --upgrade transformers\n"
      ],
      "metadata": {
        "id": "JtKwGFFFl2ln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "from google.colab import userdata\n",
        "NUERONPEDIA_API_KEY = userdata.get('neuronpedia-API')\n",
        "NGROK_API_KEY = userdata.get('ngrok')\n",
        "print('API key successfully retrieved')\n",
        "\n",
        "TEST_PROMPT = 'The spinning top at the end of movie inception means' #\n",
        "print(f\"Test prompt: {TEST_PROMPT}\")"
      ],
      "metadata": {
        "id": "fjdx4la-xg6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from sae_lens import SAE\n",
        "import requests\n",
        "\n",
        "# load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "model.config.output_hidden_states = True\n",
        "device = torch.device(\"cude\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "t9ip10xCS1Im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testt the model\n",
        "input_ids = tokenizer(TEST_PROMPT, return_tensors=\"pt\").input_ids.to(device)\n",
        "attention_mask = tokenizer(TEST_PROMPT, return_tensors=\"pt\").attention_mask.to(device)\n",
        "\n",
        "#explicitly set pad token ID\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "#generate text\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(input_ids, attention_mask=attention_mask,\n",
        "                             max_length=input_ids.shape[1]+1)\n",
        "    generated_text = tokenizer.decode(outputs[0])\n",
        "    print(generated_text)\n"
      ],
      "metadata": {
        "id": "gVbslYYHdxdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_k_tokens(prompt_text, k=3):\n",
        "    inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        next_token_logits = logits[0, -1, :]\n",
        "\n",
        "        probabilities = torch.softmax(next_token_logits, dim=-1)\n",
        "        topk_probs, topk_indices = torch.topk(probabilities, k)\n",
        "        topk_tokens = []\n",
        "        for idx, prob in zip(topk_indices.tolist(), topk_probs.tolist()):\n",
        "          token_text = tokenizer.decode([idx])\n",
        "          topk_tokens.append((token_text, prob))\n",
        "\n",
        "        return topk_tokens"
      ],
      "metadata": {
        "id": "1PR3GsX-jXWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "suggestions = get_top_k_tokens(TEST_PROMPT, k=3)\n",
        "print(f\"Prompt: {TEST_PROMPT}\")\n",
        "print(\"Top 3 next token suggestions (with probabilities):\")\n",
        "for token, prob in suggestions:\n",
        "    print(f\"{repr(token)} with probability {prob*100:.2f}%\")"
      ],
      "metadata": {
        "id": "tZKRiVIKk3gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sae, cfg, sparsity = SAE.from_pretrained(\n",
        "    release=\"gpt2-small-res-jb\",\n",
        "    sae_id=\"blocks.11.hook_resid_pre\",\n",
        "    device=str(device)\n",
        ")\n",
        "\n",
        "\n",
        "sae.to(device)\n",
        "sae.eval()\n",
        "\n",
        "print(\"SAE successfully loaded\")"
      ],
      "metadata": {
        "id": "q70FtE23mG63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_features(prompt_text, k=3):\n",
        "  inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(device)\n",
        "  with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "  hidden_states = outputs.hidden_states\n",
        "\n",
        "  resid_pre_11 = hidden_states[11]\n",
        "  final_token_activation = resid_pre_11[:, -1, :].unsqueeze(0)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    sparse_activations = sae.encode(final_token_activation)\n",
        "    sparse_activations = sparse_activations.squeeze().cpu().numpy()\n",
        "\n",
        "  top_feat_indices = sparse_activations.argsort()[-k:][::-1]\n",
        "  formatted_features = []\n",
        "  for index in top_feat_indices:\n",
        "    formatted_features.append(f\"gpt2-small/11-res-jb/{index}\")\n",
        "\n",
        "  return formatted_features"
      ],
      "metadata": {
        "id": "zU8638qYnurw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = get_features(TEST_PROMPT, k=5)\n",
        "for feature in features:\n",
        "    print(feature)"
      ],
      "metadata": {
        "id": "fzWvJ8nIpeCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "headers = {\"Authorizaton\": f\"Bearer {NUERONPEDIA_API_KEY}\"}\n",
        "url = f\"http://www.neuronpedia.org/api/feature/{features[0]}\"\n",
        "res = requests.get(url, headers=headers)\n",
        "if res.status_code == 200:\n",
        "    print(res.json())\n",
        "else:\n",
        "    print(f\"Request failed with status code {res.status_code}\")"
      ],
      "metadata": {
        "id": "XzaNw21Sp10e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_feature_info(feature_id: int, model=\"gpt2-small\", layer=\"l1-res-jb\"):\n",
        "  headers = {\"Authorizaton\": f\"Bearer {NUERONPEDIA_API_KEY}\"}\n",
        "  url = f\"http://www.neuronpedia.org/api/feature/{feature_id}\"\n",
        "  res = requests.get(url, headers=headers)\n",
        "\n",
        "  # error handling\n",
        "\n",
        "  data = res.json()\n",
        "  explanations = data.get(\"explanations\", [])\n",
        "  explanation_text = explanations[0].get(\"description\") if explanations else None\n",
        "\n",
        "  feature_info = {\n",
        "      \"index\": data.get(\"index\"),\n",
        "      \"layer\": data.get(\"layer\"),\n",
        "      \"model\": data.get(\"modelId\"),\n",
        "      \"explanation\": explanation_text,\n",
        "      \"max_activation\": data.get(\"maxActApprox\"),\n",
        "      \"positive_strings\": data.get(\"pos_str\", []),\n",
        "      \"positive_values\": data.get(\"pos_values\", []),\n",
        "      \"negative_strings\": data.get(\"neg_str\", []),\n",
        "      \"negative_values\": data.get(\"neg_values\", []),\n",
        "      \"examples\":[\n",
        "          {\n",
        "              \"tokens\": act.get(\"tokens\"),\n",
        "              \"max_value\": act.get(\"maxValue\"),\n",
        "              \"max_token\": act[\"tokens\"][act.get(\"maxValueTokenIndex\", -1)] if act.get(\"tokens\") else None\n",
        "          }\n",
        "          for act in data.get(\"activations\", [])\n",
        "      ]\n",
        "  }\n",
        "\n",
        "  return feature_info\n",
        ""
      ],
      "metadata": {
        "id": "NqtNfdT9rMH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fetch_feature_info(features[0])['explanation']"
      ],
      "metadata": {
        "id": "dS2JfAjDwp6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HJEwPIxQzteW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from model_utils import get_top_k_tokens\n",
        "from sae_utils import get_features, fetch_feature_info\n",
        "\n",
        "st.set_page_config(layout=\"wide\", page_title=\"Interpretable AI Writing Assistant\")\n",
        "\n",
        "st.title(\"Interpretable AI Writing Assistant\")\n",
        "\n",
        "# Split layout into two columns: writing (left), analysis (right)\n",
        "left, right = st.columns([2.5, 1])\n",
        "\n",
        "with left:\n",
        "    prompt = st.text_area(\"Write something:\", height=300, placeholder=\"e.g. The wizard opened the ancient...\")\n",
        "\n",
        "with right:\n",
        "    if prompt.strip():\n",
        "        st.markdown(\"### Top Predictions\")\n",
        "        preds = get_top_k_tokens(prompt, k=3)\n",
        "        for token, prob in preds:\n",
        "            st.markdown(f\"- **{token.strip()}** ({prob*100:.2f}%)\")\n",
        "\n",
        "        st.markdown(\"---\")\n",
        "        st.markdown(\"### Top Activated Features\")\n",
        "\n",
        "        feature_ids = get_features(prompt, k=3)\n",
        "\n",
        "        for fid in feature_ids:\n",
        "            info = fetch_feature_info(fid)\n",
        "\n",
        "            if info[\"explanation\"]:\n",
        "                # Display feature ID and explanation in salmon red kinda colour\n",
        "                st.markdown(\n",
        "                    f\"<span style='color:#FA8072'><b>{info['index']}: {info['explanation']}</b></span>\",\n",
        "                    unsafe_allow_html=True\n",
        "                )\n",
        "\n",
        "                # Wrap first 5 examples inside a collapsible section\n",
        "                if info[\"examples\"]:\n",
        "                    with st.expander(\"See example(s)\"):\n",
        "                        for example in info[\"examples\"][:5]:\n",
        "                            st.markdown(f\"`{example['tokens']}`\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ndDLWm4PwnYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9KNQk7u71FX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile model_utils.py\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", output_hidden_states=True)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "def get_top_k_tokens(prompt_text, k=3):\n",
        "    \"\"\"Return the top-k next token predictions and their probabilities for a given prompt.\"\"\"\n",
        "    # Tokenize the prompt\n",
        "    inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        # outputs.logits has shape [batch_size, sequence_length, vocab_size]\n",
        "        # We have batch_size=1 (single prompt), sequence_length = len(prompt tokens)\n",
        "        logits = outputs.logits  # tensor of shape (1, seq_len, vocab_size)\n",
        "    next_token_logits = logits[0, -1, :]  # scores for the next token (at the last position of sequence)\n",
        "\n",
        "    # Get probabilities (softmax) for understanding, and top-k token indices\n",
        "    probabilities = torch.softmax(next_token_logits, dim=-1)\n",
        "    topk_probs, topk_indices = torch.topk(probabilities, k)\n",
        "\n",
        "    topk_tokens = []\n",
        "    for idx, prob in zip(topk_indices.tolist(), topk_probs.tolist()):\n",
        "        token_text = tokenizer.decode([idx])\n",
        "        topk_tokens.append((token_text, prob))\n",
        "    return topk_tokens\n"
      ],
      "metadata": {
        "id": "Rd8t5s3FxQaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zf_e086n1JZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile sae_utils.py\n",
        "import torch\n",
        "from sae_lens import SAE\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import requests\n",
        "import os\n",
        "\n",
        "# Load model/tokenizer for internal use\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", output_hidden_states=True)\n",
        "model.to(\"cpu\")\n",
        "model.eval()\n",
        "\n",
        "# Load SAE\n",
        "sae, _, _ = SAE.from_pretrained(\n",
        "    release=\"gpt2-small-res-jb\",\n",
        "    sae_id=\"blocks.11.hook_resid_pre\",\n",
        "    device=\"cpu\"\n",
        ")\n",
        "sae.eval()\n",
        "\n",
        "NEURONPEDIA_API_KEY = os.environ.get(\"NEURONPEDIA_API_KEY\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def get_features(prompt_text, k=3):\n",
        "    \"\"\"\n",
        "    Extract top-k SAE features from the hidden state of the prompt.\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Run prompt through model to get hidden states\n",
        "    inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    hidden_states = outputs.hidden_states  # list of 13 tensors (0 = embeddings, 12 = post layer 11)\n",
        "\n",
        "    # Step 2: Grab hidden state at resid_pre of layer 11\n",
        "    resid_pre_11 = hidden_states[11]  # shape: [1, seq_len, hidden_dim]\n",
        "    final_token_activation = resid_pre_11[0, -1, :].unsqueeze(0)  # shape [1, 768]\n",
        "\n",
        "    # Step 3: Encode this activation through the SAE\n",
        "    with torch.no_grad():\n",
        "        sparse_activations = sae.encode(final_token_activation)  # shape: [1, num_features]\n",
        "        sparse_activations = sparse_activations.squeeze().cpu().numpy()  # shape: [num_features]\n",
        "\n",
        "    # Step 4: Extract top-k activated feature indices\n",
        "    top_feat_indices = sparse_activations.argsort()[-k:][::-1]\n",
        "    formatted_features = []\n",
        "    for index in top_feat_indices:\n",
        "        formatted_features.append(f\"gpt2-small/11-res-jb/{index}\")\n",
        "\n",
        "    return formatted_features\n",
        "\n",
        "def fetch_feature_info(feature_id: int, model='gpt2-small', layer='0-res-jb'):\n",
        "    \"\"\"\n",
        "    Fetches interpretability information about a single SAE feature from Neuronpedia API.\n",
        "\n",
        "    Parameters:\n",
        "        feature_id (int): The ID of the SAE feature.\n",
        "        model (str): The GPT-2 model ID (default is 'gpt2-small').\n",
        "        layer (str): The specific SAE layer (default is '0-res-jb').\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing metadata and interpretability signals about the feature.\n",
        "    \"\"\"\n",
        "    headers = {\"Authorization\": f\"Bearer {NEURONPEDIA_API_KEY}\"}\n",
        "    url = f\"https://www.neuronpedia.org/api/feature/{feature_id}\"\n",
        "    res = requests.get(url, headers=headers)\n",
        "    if res.status_code != 200:\n",
        "        print(f\"Error fetching feature info for {feature_id}: {res.status_code}\")\n",
        "        return {\"name\": None, \"description\": None, \"example\": None}\n",
        "    data = res.json()\n",
        "\n",
        "    explanations = data.get(\"explanations\", [])\n",
        "    explanation_text = explanations[0].get(\"description\") if explanations else None\n",
        "\n",
        "    # Parse remaining relevant info\n",
        "    feature_info = {\n",
        "        \"index\": data.get(\"index\"),\n",
        "        \"layer\": data.get(\"layer\"),\n",
        "        \"model\": data.get(\"modelId\"),\n",
        "        \"explanation\": explanation_text,\n",
        "        \"max_activation\": data.get(\"maxActApprox\"),\n",
        "        \"positive_strings\": data.get(\"pos_str\", []),\n",
        "        \"positive_values\": data.get(\"pos_values\", []),\n",
        "        \"negative_strings\": data.get(\"neg_str\", []),\n",
        "        \"negative_values\": data.get(\"neg_values\", []),\n",
        "        \"examples\": [\n",
        "            {\n",
        "                \"tokens\": act.get(\"tokens\"),\n",
        "                \"max_value\": act.get(\"maxValue\"),\n",
        "                \"max_token\": act[\"tokens\"][act.get(\"maxValueTokenIndex\", -1)] if act.get(\"tokens\") else None\n",
        "            }\n",
        "            for act in data.get(\"activations\", [])\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    return feature_info"
      ],
      "metadata": {
        "id": "-LNIMz1bxhs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!killall ngrok"
      ],
      "metadata": {
        "id": "NT7enWzJ165C"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import conf, ngrok, process\n",
        "import os\n",
        "\n",
        "from google.colab import userdata\n",
        "ngrok.set_auth_token(userdata.get('ngrok'))\n",
        "\n",
        "NEURONPEDIA_API_KEY = userdata.get('neuronpedia-API')\n",
        "os.environ[\"NEURONPEDIA_API_KEY\"] = NEURONPEDIA_API_KEY  # already loaded via Google Secrets\n",
        "\n",
        "# Start tunnel, specifying the address with the port\n",
        "public_url = ngrok.connect(addr=\"8501\")  # Pass port as part of addr\n",
        "print(f\"Streamlit app running at: {public_url}\")\n",
        "\n",
        "# Run Streamlit\n",
        "!streamlit run app.py --server.port=8501 &> /content/logs.txt &"
      ],
      "metadata": {
        "id": "n8ckE0-zx6Dy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}